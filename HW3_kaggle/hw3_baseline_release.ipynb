{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE # 效果也滿差的，只比overSample好一點\n",
    "from imblearn.combine import SMOTEENN # 效果不錯，比SMOTE好多了\n",
    "from imblearn.combine import SMOTETomek # 效果挺差的，與SMOTE差不多\n",
    "from imblearn.under_sampling import RandomUnderSampler # underSample搭配RF效果不錯，再加上pca更好一點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read input data\n",
    "'''\n",
    "train_data = pd.read_csv('training_data.csv', header=None)\n",
    "test_data = pd.read_csv('testing_data.csv', header=None)\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "# print(train_size, test_size) # 47438 25545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Deal with input data\n",
    "'''\n",
    "test_index = test_data.loc[:,0]\n",
    "test_data = test_data.drop(0, axis=1) #J:看不太懂為何把column[0]drop掉\n",
    "# print(test_data)\n",
    "\n",
    "def test_rename(x):\n",
    "    return x+1\n",
    "test_data = test_data.rename(mapper=test_rename, axis=1) # 不明原因，這邊把全部的column都+1了\n",
    "train_label = train_data.iloc[:,1]\n",
    "train_data_d = train_data.drop(labels=[0,1], axis=1)\n",
    "# print(train_data_d)\n",
    "\n",
    "\n",
    "# #J: 這邊亂試試看dropna 怪怪的 好像有bug\n",
    "# train_data_d = train_data_d.dropna(axis=0)\n",
    "# print(train_data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_df = pd.concat((train_data_d, test_data))\n",
    "# print(concated_df) #J: concat在一起後，一起處理缺失值、oneHotCoding，接著再重新分開一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature = [2,5,6,7,8,9,10,11,13,14,15,24,25,27]\n",
    "num_feature = [3,4,12,16,17,18,19,20,21,22,23,26,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Deal with missing value 可嘗試調整這裡\n",
    "Fill category feature with mode, numeric feature with mean\n",
    "'''\n",
    "# J:原始版本的填值方式\n",
    "for col in cat_feature:\n",
    "    concated_df[col] = concated_df[col].fillna(concated_df[col].mode()[0])\n",
    "for col in num_feature:\n",
    "    concated_df[col] = concated_df[col].fillna(concated_df[col].mean())\n",
    "concated_df = pd.get_dummies(concated_df, columns=cat_feature)\n",
    "    \n",
    "    \n",
    "# J:這邊我直接用drop看看。 結果失敗，連test集都有缺漏值\n",
    "# concated_df = concated_df.dropna(axis=0)\n",
    "# concated_df = pd.get_dummies(concated_df, columns=cat_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_d = concated_df.iloc[:train_size,:]\n",
    "test_data = concated_df.iloc[train_size:,:]\n",
    "# print(len(train_data_d), len(test_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train/test dataframe into numpy array\n",
    "'''\n",
    "X = train_data_d.values\n",
    "y = train_label.values\n",
    "\n",
    "'''處理Data Imbalance問題，重要!!!!'''\n",
    "# 嘗試underSample 效果不錯\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# 嘗試SMOTE 效果不太好\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "\n",
    "# 嘗試SMOTEENN 效果比SMOTE好很多 但還是比underSample差\n",
    "sme = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = sme.fit_sample(X, y)\n",
    "\n",
    "# 嘗試SMOTETomek 效果比SMOTEENN差\n",
    "# smt = SMOTETomek(random_state=42)\n",
    "# X_resampled, y_resampled = smt.fit_sample(X, y)\n",
    "\n",
    "X_test = test_data.values # J: 要預測的資料集，不要動到哦!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60897\n"
     ]
    }
   ],
   "source": [
    "print(len(X_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60897\n"
     ]
    }
   ],
   "source": [
    "print(len(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25545\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Standardlize numeric feature\n",
    "'''\n",
    "# 原本的標準化方法\n",
    "std = StandardScaler()\n",
    "std.fit(X_resampled[:,:14])\n",
    "X_resampled[:,:14] = std.transform(X_resampled[:,:14])\n",
    "X_test[:,:14] = std.transform(X_test[:,:14])\n",
    "\n",
    "\n",
    "# J: 嘗試標準化更多東西，沒比較厲害\n",
    "# std = StandardScaler()\n",
    "# std.fit(X_resampled[:, :])\n",
    "# X_resampled[:, :] = std.transform(X_resampled[:, :])\n",
    "# X_test[:, :] = std.transform(X_test[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J: 使用PCA降維\n",
    "pca = PCA(n_components=20)\n",
    "X_resampled = pca.fit_transform(X_resampled)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train model\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier # RF效果還不錯 \n",
    "# 預設使用RF 效果不錯\n",
    "clf = RandomForestClassifier(n_estimators = 105 , n_jobs = 4,random_state=10)\n",
    "clf.fit(X_resampled , y_resampled)\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Output prediction\n",
    "with open('super_easy_baseline_rf.csv','w') as f:\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for idx , p in zip(test_index , pred):\n",
    "        f.write('{},{}\\n'.format(idx,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shawn\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm # RBF SVM效果其實不錯\n",
    "# 嘗試RBF SVM\n",
    "clf = svm.SVC(kernel='rbf' ,C=2 ,random_state=0) \n",
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Output prediction\n",
    "with open('super_easy_baseline_svm.csv','w') as f:\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for idx , p in zip(test_index , pred):\n",
    "        f.write('{},{}\\n'.format(idx,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoost效果不是挺好\n",
    "# 嘗試AdaBoost\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(X_resampled , y_resampled)\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Output prediction\n",
    "with open('super_easy_baseline_ada.csv','w') as f:\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for idx , p in zip(test_index , pred):\n",
    "        f.write('{},{}\\n'.format(idx,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier # Bagging效果也不是挺好\n",
    "# 嘗試Bagging \n",
    "clf = BaggingClassifier(n_estimators = 100)\n",
    "clf.fit(X_resampled , y_resampled)\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Output prediction\n",
    "with open('super_easy_baseline_bagging.csv','w') as f:\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for idx , p in zip(test_index , pred):\n",
    "        f.write('{},{}\\n'.format(idx,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
